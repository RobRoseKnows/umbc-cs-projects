\documentclass[12pt]{article}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\parindent}{0in}
\setlength{\parskip}{\baselineskip}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{booktabs,multirow}

\title{CMSC 473 - A01}

\begin{document}

CMSC 473 Fall 2019\hfill Assignment \#1\\
Robert Rose

\hrulefill

\begin{enumerate}
\item Math for Machine Learning
  \begin{enumerate}
  \item Compute the derivative of the function $f(x) = e^(-\frac{1}{2}(x - \mu)^2)$\\
  \vspace{-2.5em}
  \paragraph{Answer} 

  \item Compute the derivative of the function $f(x) = \log(x^2 + x - 1)$\\
  \vspace{-2.5em}
  \paragraph{Answer} 

  \item Compute the derivative with respect to $x$ of the function 
        $f(x) = \log(\sum_{k=1}^{K}e^{kx^k})$, for finite, positive, integral $K$.\\
  \vspace{-2.5em}
  \paragraph{Answer} 

  \item Compute the derivative with respect to $x$ of the function
        $f(x) = \log(\prod_{k=1}^{K}e^{kx^k})$, for finite, positive, integral $K$.\\
  \vspace{-2.5em}
  \paragraph{Answer} 

  \item Given $N$ points $\{(x_n,y_n)\}_{n=1}^{N}$, compute the (partial) derivative
        $\frac{\partial J}{\partial b}$, where $J(m,b) = \sum_{n=1}^{N}( ( mx_n + b ) - y_n )^2$. 
        Note that the $N$ points $(x_n, y_n)$ can be considered constants. \textit{Hint:} the
        partial derivative $\frac{\partial J}{\partial b}$ us a deruvatuve with respect to $b$,
        where you treat all other variables as constant.\\
  \vspace{-2.5em}
  \paragraph{Answer} 

  \item For $J(m,b)$ defined on the $N$ points, as above, compute the values of $m$ and 
        $b$ that result in the gradient of $J$ being the zero vector, i.e., 
        $\nabla J = (\frac{\partial J}{\partial b}, \frac{\partial J}{\partial m})$.
        In this case, finding these values minimizes $J$. You should be able to find 
        closed-form expressions for both $m$ and $b$.\\
  \vspace{-2.5em}
  \paragraph{Answer}

  \item Given two $K$-dimensional vectors $u$ and $v$, compute the dot product $u^{\top}v$\\
  \vspace{-2.5em}
  \paragraph{Answer}

  \item Given the matrix $A = 
        \begin{pmatrix}
          4   & 2 & 0 \\
          -1  & 0 & -1
        \end{pmatrix}$, compute the values $A^{\top}A$ and $AA^{\top}$\\
  \vspace{-2.5em}
  \paragraph{Answer}

  \item For the multivariate function $f(u,v) = e^{u^{\top}v}$, where $u, v \in \mathbb{R}^K$,
        compute the gradients $\nabla_u f$ and $\nabla_v f$ \\
  \vspace{-2.5em}
  \paragraph{Answer}
  \end{enumerate}
\newpage

\item \textbf{2.10} Consider a modified version of the vacuum environment in Exercise 2.8, in which the
agent is penalized one point for each movement.
  \begin{enumerate}
  \item Can a simple reflex agent be perfectly rational for this environment? Explain.\\
  \vspace{-2.5em}
  \paragraph{Yes/No} Answer

  \item What about a reflex agent with state? Design such an agent.\\
  \vspace{-2.5em}
  \paragraph{Yes/No} Answer

  \item How do your answers to \textbf{a} and \textbf{b} change if the agentâ€™s percepts give it the clean/dirty
  status of every square in the environment?\\
  \vspace{-2.5em}
  \paragraph{Yes/No, Yes/No} Answer
  \end{enumerate}

\end{enumerate}
\end{document}