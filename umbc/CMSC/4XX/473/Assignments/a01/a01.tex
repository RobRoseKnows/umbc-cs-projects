\documentclass[12pt]{article}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\parindent}{0in}
\setlength{\parskip}{\baselineskip}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{booktabs,multirow}

\title{CMSC 473 - A01}

\begin{document}

CMSC 473 Fall 2019\hfill Assignment \#1\\
Robert Rose

\hrulefill

\begin{enumerate}
\item Math for Machine Learning
  \begin{enumerate}
  \item Compute the derivative of the function $f(x) = e^{(-\frac{1}{2}(x - \mu)^2)}$\\
  \vspace{-2.5em}
  \paragraph{Answer}  $f'(x) = - \frac{1}{2} \times 2x \times e^{(-\frac{1}{2}(x - \mu)^2)} \times (x - \mu)^2$

  \item Compute the derivative of the function $f(x) = \log(x^2 + x - 1)$\\
  \vspace{-2.5em}
  \paragraph{Answer} $\frac{(2x + 1)(x^2 + x - 1)}{x^2 + x - 1}$

  \item Compute the derivative with respect to $x$ of the function 
        $f(x) = \log(\sum_{k=1}^{K}e^{kx^k})$, for finite, positive, integral $K$.\\
  \vspace{-2.5em}
  \paragraph{Answer} $\int_{k=1}^{K}e^{kx^k}kx^{k-1}$

  \item Compute the derivative with respect to $x$ of the function
        $f(x) = \log(\prod_{k=1}^{K}e^{kx^k})$, for finite, positive, integral $K$.\\
  \vspace{-2.5em}
  \paragraph{Answer} $\int_{k=1}^{K}e^{kx^k}kx^{k-1}$

  \item Given $N$ points $\{(x_n,y_n)\}_{n=1}^{N}$, compute the (partial) derivative
        $\frac{\partial J}{\partial b}$, where $J(m,b) = \sum_{n=1}^{N}( ( mx_n + b ) - y_n )^2$. 
        Note that the $N$ points $(x_n, y_n)$ can be considered constants. \textit{Hint:} the
        partial derivative $\frac{\partial J}{\partial b}$ us a deruvatuve with respect to $b$,
        where you treat all other variables as constant.\\
  \vspace{-2.5em}
  \paragraph{Answer} $N$

  \item For $J(m,b)$ defined on the $N$ points, as above, compute the values of $m$ and 
        $b$ that result in the gradient of $J$ being the zero vector, i.e., 
        $\nabla J = (\frac{\partial J}{\partial b}, \frac{\partial J}{\partial m})$.
        In this case, finding these values minimizes $J$. You should be able to find 
        closed-form expressions for both $m$ and $b$.\\
  \vspace{-2.5em}
  \paragraph{Answer}

  \item Given two $K$-dimensional vectors $u$ and $v$, compute the dot product $u^{\top}v$\\
  \vspace{-2.5em}
  \paragraph{Answer} $u^{\top}v = u_1v_1 + u_2v_2 + \dots + u_Kv_K$

  \item Given the matrix $A = 
        \begin{pmatrix}
          4   & 2 & 0 \\
          -1  & 0 & -1
        \end{pmatrix}$, compute the values $A^{\top}A$ and $AA^{\top}$\\
  \vspace{-2.5em}
  \paragraph{Answer} $$A^{\top}A = \begin{pmatrix}
                                    17 & 8 & 1\\
                                    8  & 4 & 0\\
                                    1 & 0 & 1
                                  \end{pmatrix}$$
                    $$AA^{\top} = \begin{pmatrix}
                                    20 & -4\\
                                    -4 & 2
                                  \end{pmatrix}$$

  \item For the multivariate function $f(u,v) = e^{u^{\top}v}$, where $u, v \in \mathbb{R}^K$,
        compute the gradients $\nabla_u f$ and $\nabla_v f$ \\
  \vspace{-2.5em}
  \paragraph{Answer}
  \end{enumerate}

\item Let $X_i$ be the random variable representing the $i$th roll of a six-sided die
      and let $x_i$ be the rolled (observed) value of the $i$th roll. Assume that each
      roll is independent from one another and the die doesn't change between rolls.
      Let $x_1$,\dots,$x_4$ be the results of rolling that die 4 different times. Each 
      $x_i$ can have a value of 1, 2, 3, 4, 5, or 6.
  \begin{enumerate}
  \item If the die is fair, compute $p(x_1, x_2, x_3, x_4)$ (the probability of observing
        those four rolls in that particular order).\\
  \vspace{-2.5em}
  \paragraph{Answer} $$p(x_1, x_2, x_3, x_4) = \frac{1}{6} \times \frac{1}{6} \times \frac{1}{6} \times \frac{1}{6} = \frac{1}{1296}$$

  \item Compute the expected value of $X_i$ assuming that each outcome is equally likely.\\
  \vspace{-2.5em}
  \paragraph{Answer} 3.5

  \item Compute the expected value of $X_i$ assuming that any odd outcome is twice as likely as an even outcome.\\
  \vspace{-2.5em}
  \paragraph{Answer} 3.33\dots
  \end{enumerate}

\item Universal Dependecies on GL
  \begin{enumerate}
    \item Dev dataset has 2,002 sentences and training dataset has 12,543.
    \item Training dataset has 19,672 types and 204,607 tokens.
    \item They're all punctuation or prepositions. 'The' is the most common, followed 
          by '.' and ','.
    \item Most of them I would consider distinct, but we could potentially collapse 
          the punctuation into a single value.
    \item A handful of the words that appear only once seem to be normal words, but many
          of them are unreasonable and appear to be things like dates, times or numbers.
          I would propose dealing with this by collapsing common values such as dates and
          times into a single type for each variety. So we could still tell what they 
          are in training. Once you get up to words that appear fifty and hundred times
          though, that begins to diasppear and most of the words look normal.
    \item There are 15,840 missing types and considerably fewer tokens (25,150).
  \end{enumerate}

\end{enumerate}
\end{document}